{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shige-ta/dl_sample/blob/main/bayesian_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXgxljGSgQt-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = ''\n",
        "\n",
        "groups = ''\n",
        "X_train_ce = train.drop(target,axis=1)\n",
        "y_train = train[target]\n",
        "X_train_ce[groups] = pd.factorize(X_train_ce[groups].values)[0]"
      ],
      "metadata": {
        "id": "ysthKCntgwGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = test.drop(target,axis=1)\n",
        "y_test = test[target]\n",
        "x_test[groups] = pd.factorize(x_test[groups].values)[0]"
      ],
      "metadata": {
        "id": "j6p--ghGhURZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "id": "y3YG4QtEie_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install category_encoders"
      ],
      "metadata": {
        "id": "VtFBNiiKiq2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders\n",
        "!pip install bayesian-optimization\n",
        "!pip install optuna\n",
        "!pip install pandas"
      ],
      "metadata": {
        "id": "VXkgNv_YiwTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "# from natsort import natsorted \n",
        "import time\n",
        "import lightgbm as lgb\n",
        "import optuna.integration.lightgbm as opt_lgb\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import category_encoders as ce\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "fhut1xluianh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_params = bayes_parameter_opt_lgb(\n",
        "                                    X_train_ce, \n",
        "                                    y_train, \n",
        "                                    init_round=5, \n",
        "                                    opt_round=10, \n",
        "                                    n_folds=3, \n",
        "                                    random_seed=SEED, \n",
        "                                    n_estimators=100, \n",
        "                                    learning_rate=0.05\n",
        "                                    )"
      ],
      "metadata": {
        "id": "Na-UrtfokbHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n",
        "    params = {\n",
        "        'application':'regression',\n",
        "        'num_iterations':4000, \n",
        "        'learning_rate':0.05, \n",
        "        'early_stopping_round':100, \n",
        "        'metric':'rmse'\n",
        "        }\n",
        "\n",
        "    params[\"num_leaves\"] = round(num_leaves)\n",
        "    params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
        "    params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
        "    params['max_depth'] = round(max_depth)\n",
        "    params['lambda_l1'] = max(lambda_l1, 0)\n",
        "    params['lambda_l2'] = max(lambda_l2, 0)\n",
        "    params['min_split_gain'] = min_split_gain\n",
        "    params['min_child_weight'] = min_child_weight\n",
        "\n",
        "    cv_result = lgb.cv(\n",
        "        params, \n",
        "        train_data,\n",
        "        nfold=n_folds, \n",
        "        seed=random_seed, \n",
        "        stratified=False, \n",
        "        verbose_eval =200, \n",
        "        metrics=['rmse']\n",
        "        )\n",
        "    \n",
        "    return max(cv_result['rmse-mean'])"
      ],
      "metadata": {
        "id": "ViJS8rTojw_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "lgbBO = BayesianOptimization(\n",
        "                        lgb_eval, \n",
        "                        {\n",
        "                            'num_leaves': (24, 45),\n",
        "                            'feature_fraction': (0.1, 0.9),\n",
        "                            'bagging_fraction': (0.8, 1),\n",
        "                            'max_depth': (5, 8.99),\n",
        "                            'lambda_l1': (0, 5),\n",
        "                            'lambda_l2': (0, 3),\n",
        "                            'min_split_gain': (0.001, 0.1),\n",
        "                            'min_child_weight': (5, 50)\n",
        "                            }, \n",
        "                        random_state=SEED\n",
        ")"
      ],
      "metadata": {
        "id": "hGYBJk63jmEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=10000, learning_rate=0.05, output_process=False):\n",
        "    \n",
        "    # prepare data\n",
        "    train_data = lgb.Dataset(\n",
        "                            data=X,  ### 適宜要修正\n",
        "                            label=y,   ### 適宜要修正\n",
        "                            free_raw_data=False\n",
        "                            )\n",
        "\n",
        "    # parameters\n",
        "    def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n",
        "        \n",
        "        params = {\n",
        "            'application':'regression',\n",
        "            'num_iterations': n_estimators, \n",
        "            'learning_rate':learning_rate, \n",
        "            'early_stopping_round':100, \n",
        "            'metric':'rmse'\n",
        "            }\n",
        "\n",
        "        params[\"num_leaves\"] = int(round(num_leaves))\n",
        "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
        "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
        "        params['max_depth'] = int(round(max_depth))\n",
        "        params['lambda_l1'] = max(lambda_l1, 0)\n",
        "        params['lambda_l2'] = max(lambda_l2, 0)\n",
        "        params['min_split_gain'] = min_split_gain\n",
        "        params['min_child_weight'] = min_child_weight\n",
        "\n",
        "        cv_result = lgb.cv(\n",
        "            params, train_data, \n",
        "            nfold=n_folds, \n",
        "            seed=random_seed, \n",
        "            stratified=False, \n",
        "            verbose_eval =200, \n",
        "            metrics=['rmse']\n",
        "            )\n",
        "        \n",
        "        return max(cv_result['rmse-mean'])\n",
        "\n",
        "    lgbBO = BayesianOptimization(lgb_eval, \n",
        "                                            {'num_leaves': (24, 45),\n",
        "                                            'feature_fraction': (0.1, 0.9),\n",
        "                                            'bagging_fraction': (0.8, 1),\n",
        "                                            'max_depth': (5, 8.99),\n",
        "                                            'lambda_l1': (0, 5),\n",
        "                                            'lambda_l2': (0, 3),\n",
        "                                            'min_split_gain': (0.001, 0.1),\n",
        "                                            'min_child_weight': (5, 50)}, random_state=0)\n",
        "    # optimize\n",
        "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
        "    \n",
        "    # output optimization process\n",
        "    if output_process==True: lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n",
        "    \n",
        "    # return best parameters\n",
        "    return lgbBO.res[0][\"params\"]"
      ],
      "metadata": {
        "id": "2robjeCIkIRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "\n",
        "groups = X_train_ce[group]\n",
        "\n",
        "lgb_results = {}                                    # 学習の履歴を入れる入物\n",
        "\n",
        "opt_params[\"application\"] = \"regression\" # 回帰を追加\n",
        "opt_params[\"metrics\"] = \"rmse\" # 評価指標を追加\n",
        "opt_params[\"num_leaves\"] = int(round(opt_params[\"num_leaves\"])) # 整数に変換\n",
        "opt_params[\"max_depth\"] = int(round(opt_params[\"max_depth\"])) # 整数に変換\n",
        "\n",
        "cv_result_bayes = []\n",
        "\n",
        "for train_index, test_index in gkf.split(X_train_ce, y_train, groups):\n",
        "    X_train_gkf, X_test_gkf = X_train_ce.iloc[train_index], X_train_ce.iloc[test_index]\n",
        "    y_train_gkf, y_test_gkf = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "    # データセットを登録\n",
        "    lgb_train = lgb.Dataset(X_train_gkf, y_train_gkf)\n",
        "    lgb_test = lgb.Dataset(X_test_gkf, y_test_gkf, reference=lgb_train)\n",
        "\n",
        "    lgb_results = {}                                    # 学習の履歴を入れる入物\n",
        "\n",
        "    model = lgb.train(\n",
        "                    params=opt_params,           # ベストパラメータをセット\n",
        "                    train_set=lgb_train,              # 訓練データを訓練用にセット\n",
        "                    valid_sets=[lgb_train, lgb_test], # 訓練データとテストデータをセット\n",
        "                    valid_names=['Train', 'Test'],    # データセットの名前をそれぞれ設定\n",
        "                    num_boost_round=100,              # 計算回数\n",
        "                    early_stopping_rounds=50,         # アーリーストッピング設定\n",
        "                    evals_result=lgb_results,\n",
        "                    verbose_eval=-1,                           # ログを最後の1つだけ表示\n",
        "                    )\n",
        "    \n",
        "    # 損失推移を表示\n",
        "    loss_train = lgb_results['Train']['rmse']\n",
        "    loss_test = lgb_results['Test']['rmse']   \n",
        "    \n",
        "    fig = plt.figure()\n",
        "    \n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('logloss')\n",
        "\n",
        "#     plt.title(f\"fold:{fold}\")\n",
        "    plt.plot(loss_train, label='train loss')\n",
        "    plt.plot(loss_test, label='test loss')\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # 推論\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    # 評価\n",
        "    # rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "    # cv_result_bayes.append(rmse)\n",
        "\n",
        "print(\"RMSE:\", cv_result_bayes)\n",
        "print(\"RMSE:\", np.mean(cv_result_bayes))"
      ],
      "metadata": {
        "id": "ISAXFBeihhr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSq4qjJViT3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}